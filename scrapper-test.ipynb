{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from db_setup import get_database_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your models\n",
    "from models import Workday_Company_Data  \n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Establish a connection to your PostgreSQL database\n",
    "engine = get_database_engine()\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Fetching the company_info data from the database\n",
    "company_data = session.query(Workday_Company_Data).all()\n",
    "\n",
    "# Converting the fetched data into a DataFrame\n",
    "company_df = pd.DataFrame([(c.Company_ID, c.Company_Name, c.Company_URL) for c in company_data],\n",
    "                          columns=['company_id', 'company_name', 'company_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_company_url https://pru.wd5.myworkdayjobs.com\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"e9820f2cfafa43aa0deb9cc02e60ef59\", element=\"f.B5275E248B8DEEB9FA0FBEBFF41D6F46.d.EEC9C1ABBBC5D1A91A53B2C59FDECF9E.e.19\")>\n",
      "jobTitle Senior DevSecOps Engineer\n",
      "jobTitle PGIM Quantitative Solutions, Investment VP, Multi-Asset Portfolio Manager\n",
      "jobTitle PGIM Fixed Income: Project Manager (Hybrid/Newark, NJ)\n",
      "jobTitle Actuarial Associate\n",
      "jobTitle Associate Actuary\n",
      "jobTitle PGIM Private Capital - Manager, Business Group Risk Management\n",
      "jobTitle Sales Support Administrator\n",
      "jobTitle PGIM Director, Third-Party Risk Management (Hybrid-Newark, NJ/Tampa,FL)\n",
      "jobTitle Business Analyst - Financial Management Transformation (Hybrid)\n",
      "jobTitle Sr Business Analyst - Financial Management Transformation (Hybrid)\n",
      "jobTitle Lead, Infrastructure Developer\n",
      "jobTitle PGIM Fixed Income: Sr. Investment Analyst – Securitized Products (Hybrid/Newark, NJ)\n",
      "jobTitle Vice President, Tech Lead - API Platform\n",
      "jobTitle Solution Train Engineer\n",
      "jobTitle PGIM Real Estate | Investment Analyst, Asset Management (Singapore)\n",
      "jobTitle Specialist, Business Systems Analysis | PGIM Private Capital\n",
      "jobTitle Project Management Director | PGIM Private Capital\n",
      "jobTitle Associate Actuary (\n",
      "jobTitle Investment Vice President, Chief Investment Officer, Gibraltar Re and Lotus Re\n",
      "jobTitle Manager, ADA Governance\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"e9820f2cfafa43aa0deb9cc02e60ef59\", element=\"f.B5275E248B8DEEB9FA0FBEBFF41D6F46.d.EEC9C1ABBBC5D1A91A53B2C59FDECF9E.e.19\")>\n",
      "jobTitle Senior DevSecOps Engineer\n",
      "jobTitle PGIM Quantitative Solutions, Investment VP, Multi-Asset Portfolio Manager\n",
      "jobTitle PGIM Fixed Income: Project Manager (Hybrid/Newark, NJ)\n",
      "jobTitle Actuarial Associate\n",
      "jobTitle Associate Actuary\n",
      "jobTitle PGIM Private Capital - Manager, Business Group Risk Management\n",
      "jobTitle Sales Support Administrator\n",
      "jobTitle PGIM Director, Third-Party Risk Management (Hybrid-Newark, NJ/Tampa,FL)\n",
      "jobTitle Business Analyst - Financial Management Transformation (Hybrid)\n",
      "jobTitle Sr Business Analyst - Financial Management Transformation (Hybrid)\n",
      "jobTitle Lead, Infrastructure Developer\n",
      "jobTitle PGIM Fixed Income: Sr. Investment Analyst – Securitized Products (Hybrid/Newark, NJ)\n",
      "jobTitle Vice President, Tech Lead - API Platform\n",
      "jobTitle Solution Train Engineer\n",
      "jobTitle PGIM Real Estate | Investment Analyst, Asset Management (Singapore)\n",
      "jobTitle Specialist, Business Systems Analysis | PGIM Private Capital\n",
      "jobTitle Project Management Director | PGIM Private Capital\n",
      "jobTitle Associate Actuary (\n",
      "jobTitle Investment Vice President, Chief Investment Officer, Gibraltar Re and Lotus Re\n",
      "jobTitle Manager, ADA Governance\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"e9820f2cfafa43aa0deb9cc02e60ef59\", element=\"f.B5275E248B8DEEB9FA0FBEBFF41D6F46.d.EEC9C1ABBBC5D1A91A53B2C59FDECF9E.e.19\")>\n"
     ]
    }
   ],
   "source": [
    "company_df=company_df.head(1)\n",
    "# Implement your scraping logic here for the current company\n",
    "job_data = []\n",
    "# Assuming you have a DataFrame named 'company_df' containing company information\n",
    "for index, company_row in company_df.iterrows():\n",
    "    # Assuming you have retrieved the company_id, company_name, and company_url from the DataFrame\n",
    "    company_id = company_row['company_id']\n",
    "    company_name = company_row['company_name']\n",
    "    company_url = company_row['company_url']\n",
    "\n",
    "    # Extract the base URL from the company URL\n",
    "    # if company_url.endswith('/en-US/Careers'):\n",
    "    #     base_company_url = company_url[:-len('/en-US/Careers')]\n",
    "    # elif company_url.endswith('/en-US'):  # Check if company_url ends with \"/en-US\"\n",
    "    #     base_company_url = company_url[:-len('/en-US')]  # Remove \"/en-US\" if it exists\n",
    "    # elif company_url.endswith('/'):\n",
    "    #     base_company_url = company_url.rsplit('/', 1)[0]\n",
    "    # else :\n",
    "    #     base_company_url=company_url\n",
    "\n",
    "    base_company_url = company_url.split('.com')[0] + '.com'\n",
    "    print(\"base_company_url\",base_company_url)\n",
    "\n",
    "    \n",
    "    # Navigate to the company's website\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(company_url)\n",
    "\n",
    "\n",
    "    page_count = 0\n",
    "    while page_count < 3:\n",
    "        # Get the page source\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find all job postings on the current page\n",
    "        job_postings = soup.find_all(\"li\", class_=\"css-1q2dra3\")\n",
    "\n",
    "        # Extract job data from each posting\n",
    "        for job in job_postings:\n",
    "            try:\n",
    "                job_title_element = job.find('a', class_='css-19uc56f')\n",
    "                job_title = job_title_element.text.strip()\n",
    "                print('jobTitle',job_title)\n",
    "                job_url = job_title_element['href']\n",
    "                \n",
    "\n",
    "                location_element = job.find('dt', string='locations').find_next_sibling('dd')\n",
    "                location = location_element.text.strip()\n",
    "                \n",
    "                \n",
    "                posting_date_element = job.find('dt', string=lambda text: text in ['posted on', 'postedOn']).find_next_sibling('dd')\n",
    "                posting_date = posting_date_element.text.strip()\n",
    "                \n",
    "\n",
    "                job_id_element = job.find('li', class_='css-h2nt8k')\n",
    "                job_req_id = job_id_element.text.strip()\n",
    "                \n",
    "\n",
    "                \n",
    "                if posting_date in ['Posted Today', 'Posted Yesterday']:\n",
    "                    full_job_url = f\"{base_company_url}{job_url}\"\n",
    "            \n",
    "                    job_data.append({\n",
    "                        'company_id': company_id,\n",
    "                        'company_name': company_name,\n",
    "                        'company_url': company_url,\n",
    "                        'job_title': job_title,\n",
    "                        'job_url':  full_job_url,\n",
    "                        'location': location,\n",
    "                        'posting_date': posting_date,\n",
    "                        'job_req_id': job_req_id\n",
    "                    })\n",
    "                    \n",
    "                else:\n",
    "                    continue\n",
    "            except AttributeError as e:\n",
    "                print(\"Error occurred while extracting job data:\", e)\n",
    "                continue\n",
    "\n",
    "        # Increment page count\n",
    "        page_count += 1\n",
    "        # Look for the next page button\n",
    "        # next_button_xpath = f'//li[contains(@class, \"css-1j096s0\")]/button[text()=\"{page_count + 1}\"]'\n",
    "        try: \n",
    "            next_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(@class, 'css-1j096s0')]/button[contains(@aria-label, 'page')]\")))\n",
    "            print(next_button)\n",
    "            # next_button = driver.find_element(By.XPATH, next_button)\n",
    "\n",
    "            # If there is no next page button, break the loop\n",
    "            if not next_button or page_count >= 3:\n",
    "                break\n",
    "            # Click the next page button\n",
    "            next_button.click()\n",
    "            \n",
    "            # Wait for the page to load\n",
    "            WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.ID, \"loading-indicator\")))  # Replace \"loading-indicator\" with actual ID if applicable\n",
    "        except Exception as e:\n",
    "            print(f\"Error navigating to the next page: {e}\")\n",
    "            continue  # Break the loop if unable to find the next button or any other exception occurs\n",
    "\n",
    "    # Quit the driver\n",
    "    driver.quit()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job = pd.DataFrame(job_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_job.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
